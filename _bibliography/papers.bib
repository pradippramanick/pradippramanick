@inproceedings{sarkar2023tage,
  title={tagE: Enabling an Embodied Agent to Understand Human Instructions},
  author={Sarkar, Chayan and Mitra, Avik and Pramanick, Pradip and Nayak, Tapas},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={8846--8857},
  year={2023},
   pdf = "https://aclanthology.org/2023.findings-emnlp.593.pdf",
   html = "https://aclanthology.org/2023.findings-emnlp.593",
   publisher = "Association for Computational Linguistics",
   code = {https://github.com/csarkar/tagE},
   abstract = "Natural language serves as the primary mode of communication when an intelligent agent with a physical presence engages with human beings. While a plethora of research focuses on natural language understanding (NLU), encompassing endeavors such as sentiment analysis, intent prediction, question answering, and summarization, the scope of NLU directed at situations necessitating tangible actions by an embodied agent remains limited. The inherent ambiguity and incompleteness inherent in natural language present challenges for intelligent agents striving to decipher human intention. To tackle this predicament head-on, we introduce a novel system known as task and argument grounding for Embodied agents (tagE). At its core, our system employs an inventive neural network model designed to extract a series of tasks from complex task instructions expressed in natural language. Our proposed model adopts an encoder-decoder framework enriched with nested decoding to effectively extract tasks and their corresponding arguments from these intricate instructions. These extracted tasks are then mapped (or grounded) to the robot{'}s established collection of skills, while the arguments find grounding in objects present within the environment. To facilitate the training and evaluation of our system, we have curated a dataset featuring complex instructions. The results of our experiments underscore the prowess of our approach, as it outperforms robust baseline models."
}
@inproceedings{10.1145/3568294.3580129,
author = {Pramanick, Pradip and Sarkar, Chayan},
title = {Utilizing Prior Knowledge to Improve Automatic Speech Recognition in Human-Robot Interactive Scenarios},
year = {2023},
isbn = {9781450399708},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
html = {https://doi.org/10.1145/3568294.3580129},
doi = {10.1145/3568294.3580129},
abstract = {The prolificacy of human-robot interaction not only depends on a robot's ability to understand the intent and content of the human utterance but also gets impacted by the automatic speech recognition (ASR) system. Modern ASR can provide highly accurate (grammatically and syntactically) translation. Yet, the general purpose ASR often misses out on the semantics of the translation by incorrect word prediction due to open-vocabulary modeling. ASR inaccuracy can have significant repercussions as this can lead to a completely different action by the robot in the real world. Can any prior knowledge be helpful in such a scenario? In this work, we explore how prior knowledge can be utilized in ASR decoding. Using our experiments, we demonstrate how our system can significantly improve ASR translation for robotic task instruction.},
booktitle = {Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {471â€“475},
numpages = {5},
keywords = {asr, cognitive robot, embodied agent, hri, robotics knowledge},
location = {Stockholm, Sweden},
series = {HRI '23},
pdf = "https://pradippramanick.github.io/assets/pdf/3568294.3580129.pdf"
}
@inproceedings{pramanick-sarkar-2022-visual,
    title = "Can Visual Context Improve Automatic Speech Recognition for an Embodied Agent?",
    author = "Pramanick, Pradip  and
      Sarkar, Chayan",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    html = "https://aclanthology.org/2022.emnlp-main.127",
    doi = "10.18653/v1/2022.emnlp-main.127",
    pdf = "https://aclanthology.org/2022.emnlp-main.127.pdf",
    pages = "1946--1957",
    abstract = "The usage of automatic speech recognition (ASR) systems are becoming omnipresent ranging from personal assistant to chatbots, home, and industrial automation systems, etc. Modern robots are also equipped with ASR capabilities for interacting with humans as speech is the most natural interaction modality. However, ASR in robots faces additional challenges as compared to a personal assistant. Being an embodied agent, a robot must recognize the physical entities around it and therefore reliably recognize the speech containing the description of such entities. However, current ASR systems are often unable to do so due to limitations in ASR training, such as generic datasets and open-vocabulary modeling. Also, adverse conditions during inference, such as noise, accented, and far-field speech makes the transcription inaccurate. In this work, we present a method to incorporate a robot{'}s visual information into an ASR system and improve the recognition of a spoken utterance containing a visible entity. Specifically, we propose a new decoder biasing technique to incorporate the visual context while ensuring the ASR output does not degrade for incorrect context. We achieve a 59{\%} relative reduction in WER from an unmodified ASR system.",
}

