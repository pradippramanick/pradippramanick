@misc{p1,
  abbr={US, EP},
  title={Method and system for visual context aware automatic speech recognition},
  author={Chayan Sarkar and Pradip Pramanick and Ruchira Singh},
  year={2024},
  note={US Patent App. 18/333,983, EP4325482A1},
  url={https://patents.google.com/patent/US20240038224A1/en},
  html={https://patents.google.com/patent/US20240038224A1/en},
  selected={true},
  abstract="Accuracy of transcript is of foremost importance in Automatic Speech Recognition (ASR). State of the art system mostly rely on spelling correction based contextual improvement in ASR, which is generally a static vocabulary based biasing approach. Embodiments of the present disclosure provide a method and system for visual context aware ASR. The method provides biasing using shallow fusion biasing approach with a modified beam search decoding technique, which introduces a non-greedy pruning strategy to allow biasing at the sub-word level. The biasing algorithm brings in the visual context of the robot to the speech recognizer based on a dynamic biasing vocabulary, improving the transcription accuracy. The dynamic biasing vocabulary, comprising objects in a current environment accompanied by their self and relational attributes, is generated using a bias prediction network that explicitly adds label to objects, which are detected and captioned via a state of the art dense image captioning network."
}


@misc{sarkar2024methods,
abbr={US, EP},
  title={Methods and systems for disambiguation of referred objects for embodied agents},
  author={Sarkar, Chayan and Pramanick, Pradip and Bhowmick, Brojeshwar and Roychoudhury, Ruddra Dev and Sayan, PAUL},
  year={2024},
  note={US Patent App. 18/207,836,  EP4303740A1},
  html="https://patents.google.com/patent/US20240013538A1/en",
   selected={true},
  abstract="This disclosure addresses the unresolved problems of tackling object disambiguation task for an embodied agent. The embodiments of present disclosure provide a method and system for disambiguation of referred objects for embodied agents. With a phrase-to-graph network disclosed in the system of the present disclosure, any natural language object description indicating the object disambiguation task can be converted into a semantic graph representation. This not only provides a formal representation of the referred object and object instances but also helps to find an ambiguity in disambiguating the referred object using a real-time multi-view aggregation algorithm. The real-time multi-view aggregation algorithm processes multiple observations from an environment and finds the unique instances of the referred object. The method of the present disclosure demonstrates significant improvement in qualifying ambiguity detection with accurate, context-specific information so that it is sufficient for a user to come up with a reply towards disambiguation."
}

@misc{banerjee2023telepresence,
   abbr={US},
  title={Telepresence robots having cognitive navigation capability},
  author={Banerjee, Snehasis and Pramanick, Pradip and Sarkar, Chayan and Bhattacharyya, Abhijan and Ashis, SAU and Anand, Kritika and Roychoudhury, Ruddra Dev and Bhowmick, Brojeshwar},
  year={2023},
  note={US Patent App. 17/814,306},
  html="https://patents.google.com/patent/US20230213941A1/en",
  abstract="
The embodiments of present disclosure herein address unresolved problem of cognitive navigation strategies for a telepresence robotic system. This includes giving instruction remotely over network to go to a point in an indoor space, to go an area, to go to an object. Also, human robot interaction to give and understand interaction is not integrated in a common telepresence framework. The embodiments herein provide a telepresence robotic system empowered with a smart navigation which is based on in situ intelligent visual semantic mapping of the live scene captured by a robot. It further presents an edge-centric software architecture of a teledrive comprising a speech recognition based HRI, a navigation module and a real-time WebRTC based communication framework that holds the entire telepresence robotic system together. Additionally, the disclosure provides a robot independent API calls via device driver ROS, making the offering hardware independent and capable of running in any robot.
"
}

@misc{banerjee2023system,
abbr={US, EP},
  title={System and method for ontology guided indoor scene understanding for cognitive robotic tasks},
  author={Banerjee, Snehasis and Purushothaman, Balamuralidhar and Pramanick, Pradip and Sarkar, Chayan},
  year={2023},
  publisher={Google Patents},
  note={US Patent App. 17/815,109, EP4170449C0 (Granted)},
  html="https://patents.google.com/patent/US20230162494A1/en",
  abstract="Existing cognitive robotic applications follow a practice of building specific applications for specific use cases. However, the knowledge of the world and the semantics are common for a robot for multiple tasks. In this disclosure, to enable usage of knowledge across multiple scenarios, a method and system for ontology guided indoor scene understanding for cognitive robotic tasks is described where in scenes are processed based on techniques filtered based on querying ontology with relevant objects in perceived scene to generate a semantically rich scene graph. Herein, an initially manually created ontology is updated and refined in online fashion using external knowledge-base, human robot interaction and perceived information. This knowledge helps in semantic navigation, aids in speech, and text based human robot interactions."
}

